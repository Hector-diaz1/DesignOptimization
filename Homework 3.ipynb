{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "harmful-logging",
   "metadata": {},
   "source": [
    "### Problem 1 (50 points) \n",
    "\n",
    "Vapor-liquid equilibria data are correlated using two adjustable parameters $A_{12}$ and $A_{21}$ per binary\n",
    "mixture. For low pressures, the equilibrium relation can be formulated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p = & x_1\\exp\\left(A_{12}\\left(\\frac{A_{21}x_2}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{water}^{sat}\\\\\n",
    "& + x_2\\exp\\left(A_{21}\\left(\\frac{A_{12}x_1}{A_{12}x_1+A_{21}x_2}\\right)^2\\right)p_{1,4 dioxane}^{sat}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here the saturation pressures are given by the Antoine equation\n",
    "\n",
    "$$\n",
    "\\log_{10}(p^{sat}) = a_1 - \\frac{a_2}{T + a_3},\n",
    "$$\n",
    "\n",
    "where $T = 20$($^{\\circ}{\\rm C}$) and $a_{1,2,3}$ for a water - 1,4 dioxane\n",
    "system is given below.\n",
    "\n",
    "|             | $a_1$     | $a_2$      | $a_3$     |\n",
    "|:------------|:--------|:---------|:--------|\n",
    "| Water       | 8.07131 | 1730.63  | 233.426 |\n",
    "| 1,4 dioxane | 7.43155 | 1554.679 | 240.337 |\n",
    "\n",
    "\n",
    "The following table lists the measured data. Recall that in a binary system $x_1 + x_2 = 1$.\n",
    "\n",
    "|$x_1$ | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |\n",
    "|:-----|:--------|:---------|:--------|:-----|:-----|:-----|:-----|:-----|:-----|:-----|:-----|\n",
    "|$p$| 28.1 | 34.4 | 36.7 | 36.9 | 36.8 | 36.7 | 36.5 | 35.4 | 32.9 | 27.7 | 17.5 |\n",
    "\n",
    "Estimate $A_{12}$ and $A_{21}$ using data from the above table: \n",
    "\n",
    "1. Formulate the least square problem; \n",
    "2. Since the model is nonlinear, the problem does not have an analytical solution. Therefore, solve it using the gradient descent or Newton's method implemented in HW1; \n",
    "3. Compare your optimized model with the data. Does your model fit well with the data?\n",
    "\n",
    "---\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "Solve the following problem using Bayesian Optimization:\n",
    "$$\n",
    "    \\min_{x_1, x_2} \\quad \\left(4-2.1x_1^2 + \\frac{x_1^4}{3}\\right)x_1^2 + x_1x_2 + \\left(-4 + 4x_2^2\\right)x_2^2,\n",
    "$$\n",
    "for $x_1 \\in [-3,3]$ and $x_2 \\in [-2,2]$. A tutorial on Bayesian Optimization can be found [here](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "divine-setup",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c61f84d51142>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# A simple example of using PyTorch for gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# A simple example of using PyTorch for gradient descent\n",
    "\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Define a variable, make sure requires_grad=True so that PyTorch can take gradient with respect to this variable\n",
    "A = Variable(t.tensor([1.0, 1.0]), requires_grad=True)\n",
    "\n",
    "\n",
    "#psat calculation \n",
    "p_water=17.4733\n",
    "p_14dioxane=28.8241\n",
    "x2=t.zeros(11)\n",
    "p_a=t.zeros(11)\n",
    "x1=t.tensor([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "p=t.tensor([28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5])\n",
    "for k in range(11):\n",
    "    x2[k]=1-x1[k]\n",
    "    p_a=((x1 * t.exp (A[0] * (((A[1] * x2) / (A[0] * x1 + A[1] * x2)))**2)*p_water) + (x2 * t.exp( (A[1] * (((A[0] * x1) / (A[0] * x1 + A[1] * x2)))**2))*p_14dioxane))\n",
    "\n",
    "# Define a loss\n",
    "\n",
    "loss = t.sum((p_a-p)**2)\n",
    "\n",
    "# Take gradient\n",
    "loss.backward()\n",
    "\n",
    "# Check the gradient. numpy() turns the variable from a PyTorch tensor to a numpy array.\n",
    "A.grad.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the gradient at a different x.\n",
    "A.data = t.tensor([2.0, 1.0])\n",
    "loss =t.sum (((x1 * t.exp (A[0] * (((A[1] * x2) / (A[0] * x1 + A[1] * x2)))**2)*p_water + x2 * (t.exp( (A[1] * ((A[0] * x1) / (A[0] * x1 + A[1] * x2))**2))*p_14dioxane))-p)**2)\n",
    "\n",
    "loss.backward()\n",
    "A.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a code for gradient descent without line search\n",
    "\n",
    "import torch as t\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "A = Variable(t.tensor([3.0, 3.0]), requires_grad=True)\n",
    "\n",
    "# Fix the step size\n",
    "a=.001\n",
    "\n",
    "# Start gradient descent\n",
    "for i in range(10000):  # TODO: change the termination criterion\n",
    "\n",
    "    p_a=((x1 * t.exp (A[0] * (((A[1] * x2) / (A[0] * x1 + A[1] * x2)))**2)*p_water) + (x2 * t.exp( (A[1] * (((A[0] * x1) / (A[0] * x1 + A[1] * x2)))**2))*p_14dioxane)) \n",
    "    loss = t.sum((p_a-p)**2)\n",
    "    loss.backward()\n",
    "\n",
    "    # no_grad() specifies that the operations within this context are not part of the computational graph, i.e., we don't need the gradient descent algorithm itself to be differentiable with respect to x\n",
    "    with t.no_grad():\n",
    "        A -= a * A.grad\n",
    "        \n",
    "        # need to clear the gradient at every step, or otherwise it will accumulate...\n",
    "        A.grad.zero_()\n",
    "\n",
    "print(A.data.numpy())\n",
    "print(loss.data.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6682c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_sol=t.zeros(11)\n",
    "for j in range(11):\n",
    "    p_a[j]=((x1[j] * t.exp ( 1.9584156* (((1.9584156 * x2[j]) / (1.9584156 * x1[j] + 1.6891813 * x2[j])))**2)*p_water) + (x2[j] * t.exp( (1.6891813 * (((1.9584156 * x1[j]) / (1.9584156 * x1[j] + 1.6891813 * x2[j])))**2))*p_14dioxane)) \n",
    "    error_sol[j]=t.abs((p_a[j]-p[j])/p[j])*100\n",
    "    total_error=t.sum(error_sol)/11\n",
    "print(error_sol)\n",
    "print(total_error)\n",
    "print(p_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae12cf",
   "metadata": {},
   "source": [
    "The solution for A12 and A21 are 1.9584156 1.6891813, respectively. Comparing this to the data collected we can see that the solution is a better approximation for the outer bounds were we have a small percent error and an averaged percent error of 6.15% which is good for this type of model. Notice that no line search algrothim was used becuase the percent error is small using other techiniques might take longer to code and might not imporove the solution significanlty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b413b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process as gp\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data, target = make_classification(n_samples=2500,\n",
    "                                   n_features=45,\n",
    "                                   n_informative=15,\n",
    "                                   n_redundant=5)\n",
    "\n",
    "def sample_loss(x):\n",
    "    total = np.array([])\n",
    "    for x_i in x:\n",
    "        total = np.append(total, ((4 - 2.1*x[0]**2 + ((x[0]**4)/3))*x[0]**2 + x[0]*x[1] + (-4 + 4*x[1]**2)*x[1]**2) )\n",
    "    \n",
    "    return total + np.random.randn()\n",
    "\n",
    "lambdas = np.linspace(1, -4, 25)\n",
    "gammas = np.linspace(1, -4, 20)\n",
    "\n",
    "# We need the cartesian combination of these two vectors\n",
    "param_grid = np.array([[C, gamma] for gamma in gammas for C in lambdas])\n",
    "\n",
    "real_loss = [sample_loss(params) for params in param_grid]\n",
    "\n",
    "# The maximum is at:\n",
    "param_grid[np.array(real_loss).argmax(), :]\n",
    "\n",
    "\n",
    "def bayesian_optimization(n_iters, sample_loss, xp, yp):\n",
    "  # Define the GP\n",
    "    kernel = gp.kernels.Matern()\n",
    "    model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                      alpha=1e-4,\n",
    "                                      n_restarts_optimizer=10,\n",
    "                                      normalize_y=True)\n",
    "for i in range(n_iters):\n",
    "    # Update our belief of the loss function\n",
    "        model.fit(xp, yp)\n",
    "\n",
    "    # sample_next_hyperparameter is a method that computes the arg\n",
    "    # max of the acquisition function\n",
    "        next_sample = sample_next_hyperparameter(model, yp)\n",
    "\n",
    "    # Evaluate the loss for the new hyperparameters\n",
    "        next_loss = sample_loss(next_sample)\n",
    "\n",
    "    # Update xp and yp\n",
    "    \n",
    "\n",
    "\n",
    "data, target = make_classification(n_samples=2500,\n",
    "                                   n_features=45,\n",
    "                                   n_informative=15,\n",
    "                                   n_redundant=5)\n",
    "\n",
    "xp, yp = bayesian_optimisation(n_iters=30, \n",
    "                               sample_loss=sample_loss, \n",
    "                               bounds=bounds,\n",
    "                               n_pre_samples=3,\n",
    "                               random_search=100000)\n",
    "bounds = np.array([[-4, 1], [-4, 1]])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
